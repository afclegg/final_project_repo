---
title: "Final Project"
author: "Ryan Collins and Alex Clegg"
date: "4/18/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

Here are the packages that will be needed for the project:

```{r packages}
library(readr)
library(readxl)
library(tidyverse)
library(tidymodels)
library(httr)
library(jsonlite)
library(ranger)
library(h2o)
library(gridExtra)
source("R/header.true.R")
source("R/sba_api_2012.R")
source("R/sba_api_2017.R")
source("R/sba_api_2007.R")
source("R/add_year.R")
source("R/my_theme.R")
source("R/model_CART_avg.R")
source("R/model_linear_avg.R")
source("R/model_CART_total.R")
source("R/model_linear_total.R")
```

## Project Overview:

With the coronavirus pandemic having upended life as we know it, many government from around the world have instituted massive lockdowns on economic activity to slow the spread of the virus and protect vulnerable segments of the populaiton. Unfortunately, these measures also carry a significant economic costs. In the United States alone, they have resulting in #TKTKT jobs losses, a full scale retreat from consumption and put many industries and businesses in serious threat of bankrupcy and insolvency. As a result, the federal government through several rounds of legislative action have tried to stem the economic losses through several programs. One - the Paycheck Protection Program (PPP) - administered by the Small Business Administration (SBA) was intented to provide businesses under 500 employees the wherewithal to withstand the immediate economic downturn with minimual loss. The original funding stipulated was $250 billion dollars.

Within the first two weeks of the program demand was overwhelming. So much so, that funding for businesses through the program ran out within a matter of weeks and forced congress to authorize an additional $310 billion dollars for the program. While the program has had challenges with scaling, acces and eligiblity it has been somewhat of a success in providing businesses with a lifeline during this downturn. Moreover, as an intended policy, the program was designed to only provide funding to small buisnesses who retained the same level of payroll - stipulating that at minimum, 75% of funds must be used to provide payroll relief and to retain employees. As such, one primary goal was to keep people employed throughout the downturn such that when government-sanctioned lockdowns lifted, those workers would be able to return to a secure job instead of find themselves in line for additional unemployment benefitis. Whether or not this will be the result, it is likely too early to tell. 

There is one thing for certain - that the program experienced massive demand - such that funds were overwhelmed and institutions were forced to serve those who came first. This has led to the unnecessary prioritization of certain businesses over others when the object of the policy was to make this a guarentee for any business that might need it as a means to retain the same level of employment as they held prior to the pandemnic. Which raises the question - why didn't congress allocate what is likely a quarter of what is needed? 

While the COVID-19 pandemic is a classic black swan event in relation to other historic disaster events, SBA and federal agencies could perhaps have been better prepared to predict the specific demand needed for the program. It is the goal of this project to predict what was the average loan amount for the Paycheck Protection Program based upon previous disaster loan approvals. While this is not an apples to apples comparison, the hope is that we find a generally useful guide for loan pracitioners and understand just how much of an outlier this crisis has and continues to be.

## Data for consideration:

### Dependent Variable:

The data we will be using for this project come from multiple sources but our primary interest is in the historical SBA disaster loan data available from SBA, which cataloges the specific loss and approved loan amounts to areas impacted by natural disasters between the years of 2008 and 2018. All relevant data can be found and downloaded [here](https://www.sba.gov/about-sba/sba-performance/open-government/digital-sba/open-data/open-data-sources). 

```{r reading in SBA disaster data, echo = F, warning = F}
#first for the disaster loan-size:
sba_sandy <- read_excel("data/SBA_Disaster_Loan_Data_Superstorm_Sandy_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2008 <- read_excel("data/SBA_Disaster_Loan_Data_FY08_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2009 <- read_excel("data/SBA_Disaster_Loan_Data_FY09_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2010 <- read_excel("data/SBA_Disaster_Loan_Data_FY10_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2011 <- read_excel("data/SBA_Disaster_Loan_Data_FY11_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2012 <- read_excel("data/SBA_Disaster_Loan_Data_FY12_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2013 <- read_excel("data/SBA_Disaster_Loan_Data_FY13_simple.xlsx", 
                        col_types = c("numeric", "numeric", "numeric",
                                      "text", "text", "numeric", "text", 
                                      "text", "numeric", "numeric", "numeric", 
                                      "numeric", "numeric", "numeric", 
                                      "numeric"))
sba_2014 <- read_excel("data/SBA_Disaster_Loan_Data_FY14_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2015 <- read_excel("data/SBA_Disaster_Loan_Data_FY15_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2016 <- read_excel("data/SBA_Disaster_Loan_Data_FY16_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2017 <- read_excel("data/SBA_Disaster_Loan_Data_FY17_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))
sba_2018 <- read_excel("data/SBA_Disaster_Loan_Data_FY18_simple.xlsx",
                       col_types = c("numeric", "numeric", "numeric", 
                                     "text", "text", "numeric", "text", 
                                     "text", "numeric", "numeric", "numeric", 
                                     "numeric", "numeric", "numeric", 
                                     "numeric"))

```

For our dependent variable, we are looking at the specific "Approved Content Loan Amount," available within the SBA data. SBA disaster dataset includes several data sources of which approved content, we believe, to be the most comparable. Specifically, the data include verified losse and approved amounts. The losses and amounts are separated into "total", "real estate", and "content." We believe that content amount and content loss are important variables because the account for the total loss/approved amount related to the non-physical property related damages including inventory, equipment and other goods/services related loss.  

which the specific amount approved by SBA for the content a business lost in the disaster. We believe this is a useful proxy for PPP loan amounts provided they are only eligble to be used for payroll and some fixed costs. We are also looking at the SBA's Economic Injury Disaster Loan amounts as they are more comparable to the current circunstances businesses are facing with COVID.

**Michael Bay Effect**

We are also adding in an additional indicator variable to demarcate unusually significant natural disasters - all of which are major hurricanes. We've specified the value one for damange caused by Hurricane Sandy (2012), Hurricane Maria (2017), and Hurricane Harvey (2017) - all of which cause unusally high amounts of damange. All other disasters will recieve a value equal to zero. It is our hope that during the model training process we may be able to amplify the impact of a disaster, such that we can show what the cost might be were a major disaster such as Sandy, Maria, or Harvey to hit the entire country like the COVID virus has.

```{r adding MBay_effect}
#adding column for major disaster variable from sandy
sba_sandy <- sba_sandy %>%
  mutate(MBay_effect = 1)

#adding column for major disaster variable from 2017 hurricanes:
x = c("FL", "TX", "PR", "VI")
sba_2017 <- sba_2017 %>%
  mutate(
MBay_effect = ifelse(sba_2017$`Damaged Property State Code`%in% x, 1 ,0))
```

```{r adding in year variable}
#adding year variable:
years <- c(2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018)

sba_2008 <- add_year(sba_2008, 1)
sba_2009 <- add_year(sba_2009, 2)
sba_2010 <- add_year(sba_2010, 3)
sba_2011 <- add_year(sba_2011, 4)
sba_2012 <- add_year(sba_2012, 5)
sba_sandy <- add_year(sba_sandy, 5)
sba_2013 <- add_year(sba_2013, 6)
sba_2014 <- add_year(sba_2014, 7)
sba_2015 <- add_year(sba_2015, 8)
sba_2016 <- add_year(sba_2016, 9)
sba_2017 <- add_year(sba_2017, 10)
sba_2018 <- add_year(sba_2018, 11)
```

```{r mergining sba disaster loan data into single dataframe}
#binding columns:
sba_merged_all<- bind_rows(sba_sandy, sba_2008, sba_2009, sba_2010, sba_2011, sba_2012, sba_2013,
                           sba_2014, sba_2015, sba_2016, sba_2017, sba_2018)
#renaming zipcode:
sba_merged_all <- sba_merged_all %>%
  rename(zipcode = `Damaged Property Zip Code`)
```

```{r finalizing MBay_effect}
#stipulating value for all other MBay_effect != 1:
sba_merged_all$MBay_effect[is.na(sba_merged_all$MBay_effect)] <- 0
```

**Conversion of Jurisdictions** 

A challenge relating to the imputation of the data from SBA has been that the data is at it's most granular at the zipcode level. While we are thankful there is some inclusion of jursidiction in the data, we had hoped it would be at the county level since our predictor varaibles come from the U.S. census and are only available at that level. The challenge arises such that we need to include a further dataset to translate disaster loan amounts from zipcode to county.

```{r reading in zipcode/FIPS conversion dataframe}
#Now will wan to read in data for fips codes:
ZIP_COUNTY_FIPS_2012_12 <- read_csv("data/ZIP-COUNTY-FIPS_2012-12.csv")

#changing names for target variable (zipcode)
ZIP_COUNTY_FIPS_2012_12 <- ZIP_COUNTY_FIPS_2012_12 %>%
  rename(zipcode = ZIP) %>%
  select(zipcode, STCOUNTYFP)
```
```{r merging zipcode/FIPS to SBA disaster data}
#merging on FIPS Codes:
sba_merged_all <- sba_merged_all %>% 
  left_join(ZIP_COUNTY_FIPS_2012_12, by = "zipcode")

#now to separate out the state and county fips codes:
sba_merged_all <- sba_merged_all %>%
  mutate(stateFIPS = substr(STCOUNTYFP, 1, 2), 
         countyFIPS = substr(STCOUNTYFP, 3, 5))
```

Moreover, as anyone who's crossed this path recognizes that zipcodes change from time to time and that they are not uniform within counties - in some instances crossing boundry lines. As such, we've created severalw weighted county-total and county-averages for variables:

  - total verified content loss
  - total content approved
  - total EIDL approved

```{r creating weighted county variables}
#creating weight for countys with multiple zipcodes:
sba_agg <- sba_merged_all %>% 
  group_by(zipcode, year) %>%
  summarise(n = n())

sba_merged_all <- left_join(sba_merged_all, 
                             sba_agg,
                             by = c("zipcode", "year"))

sba_merged_all <- sba_merged_all %>%
  mutate(weighted_content_loan = `Approved Amount Content`/n,
         weighted_loss_content = `Verified Loss Content`/n,
         weighted_eidl_amount = `Approved Amount EIDL`/n) 

sba_weighted <- sba_merged_all %>%
  group_by(countyFIPS, stateFIPS, year) %>%
  summarize(
    total_county_approved_content_loan = sum(weighted_content_loan),
    total_county_verified_content_loan = sum(weighted_loss_content),
    total_county_eidl_loan = sum(weighted_eidl_amount),
    avg_county_approved_content_loan = weighted.mean(weighted_content_loan, weight = 1/n),
    avg_county_verified_content_loan = weighted.mean(weighted_loss_content, weight = 1/n),
    avg_county_eidl_loan = weighted.mean(weighted_eidl_amount, weight = 1/n)
    )
```

We weighted our variables by creating a count variable which indicated how many times a zipcode showed up by year in our merged sba_county dataset. We were then able to remerge that into our dataset and weighted our content loss, content loan, and EIDL loan by the count variable - thus splitting those amounts by the number of counties. For example that would indicate that a loan amount from a certain zipcode wholly contained in a county would not be impact, whereas a loan amount split between two counties would effectively be halfed between the two. We then took totals and averages by county and created separate variables. These will be the finalize dependent variable under consideration. 

```{r merging weighted variables onto SBA disaster loan data}
#now merging them all onto sba_merged 
sba_merged_all <- left_join(sba_merged_all, 
                             sba_weighted,
                             by = c("countyFIPS", "stateFIPS", "year")) 

#choosing specific variables:
sba_merged_all <- sba_merged_all%>%
  select(`SBA Disaster Number`, year, zipcode, countyFIPS, stateFIPS, `Total Verified Loss`, `Verified Loss Content`, `Total Approved Loan Amount`, `Approved Amount Content`, `Approved Amount EIDL`, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan,total_county_eidl_loan, avg_county_eidl_loan, MBay_effect)
```

### Predictor Variables: 

As mentioned earlier for our predictor variables originate from the [Economic Census](https://www.census.gov/data/developers/data-sets/economic-census.html) undertaken by the U.S. Census Bureau in five year increments (2007, 2012, 2017). According to the Census, the Economic Census is the official measure of the U.S.'s business and economy. As a survey it provides the statistical benchmark for economic activity and provides granular details on busienss size, employment numbers, sales and recipets as well as other important data. For our purposes, we are interested in number of employees, number of establishments, annual payroll, first-quarter payroll, and sales or revenue,

**Predictor Variables in More Detail**

- Number of Employees: As the U.S. Treasury explains the [PPP program](https://home.treasury.gov/system/files/136/PPP--Fact-Sheet.pdf) is eligible for firms with 500 employees or less - although there is numerous instances this [might not be the case](https://www.washingtonpost.com/business/2020/05/01/sba-ppp-public-companies/). While the implementation of the program has allowed for larger firms to take advange of the program, since the goal is to provide coverage for payroll, a larger number of employees would indicate the need for larger loan sizes.

- Number of Establishments: The number of established firms in a jurisdiction influences since SBA loan performance measurea are largely [output-based](https://www.gao.gov/new.items/d08226t.pdf) as opposed to impact. As such, most SBA loan officers will likely prioritize areas with more establishments as opposed to fewers. As such, this variable is important to include. This output verus impact driven eligibility [isn't limited to only](https://crsreports.congress.gov/product/pdf/R/R43661) SBA loans.

- Annual Payroll: As mentioned above, a large goal of the PPP is to provide coverage for small buisness payroll. As such, in applying for the PPP, borrowers [must submit their previous years' annual payroll](https://www.sba.com/funding-a-business/government-small-business-loans/ppp/loan-calculator/) for underwriting of the loan. As such it's one of the most critical variables to the model.

- First Quarter Payroll: Additionally, for SBA PPP loans, monthly payroll is taken into account, such that no loan can be more than [250% of monthly payroll](https://taxfoundation.org/sba-paycheck-protection-program-cares-act/). Providing the 1st quarter payroll related to disaster may provide some clarity on what those loans would have been influenced by. This could also help to take into account some seasonanility in payroll flows.

- Sales or Revenue: This variable is important particularly for SBA content and EIDL loans where the firm needs to prove a certain amount of economic injury. [A survery](https://www.fedsmallbusiness.org/survey/2018/report-on-disaster-affected-firms) by the Federal Reserve board found that in disaster-related lending that forgone revenues (not assets) were the largest source of loss.

```{r reading and merging in Economic Census data from Census API, warning = F}
#2012 and 2007 include all the relevent NAICS data so will use for both dataframes
NAICS0712 <- c("23", "31-33", "42", "44-45", "53", "54", "56", "62", "72", "81")

sba_census_2007 <- map_df(.x = NAICS0712,
                          .f = sba_api_2007)
#Noting there is NO PAYQRT1 variable available for 2007.

#run for each year and NAICS code and the merge into three dataframes:
sba_census_2012 <- map_df(.x = NAICS0712,
                          .f = sba_api_2012)

NAICS2017 <- c("44-45", "53", "54", "56", "62", "72", "81")
#CENSUS for 2017 doesn't have Construction, Manufacturing, Whole sale retailers available.

sba_census_2017 <- map_df(.x = NAICS2017,
                          .f = sba_api_2017)

sba_census_merged <- bind_rows(sba_census_2007, sba_census_2012, sba_census_2017)
```

**North American Industry Classification System (NAICS) Limitations**

One of the challenges related to the census level data is the need to specifiy NAICS codes to avoid crashing our systems. As such, I'll create a function that specifies the NAICS codes for the top 10 industries who recieved PPP funding as of April 13th, 2020. Those industries are: 

- 23: Consturction 
- 31-33: Manufacturing:
- 42: Whole Sale
- 44-45:Rtail Trade
- 53: Real Estate Rental and Leasing
- 54: Professional, Scientific and Technical Services
- 56: Administrative and Support and Waste Management and Remediation Services
- 62: Health Care and Social Assistance
- 72: Accomodation and Food Services 
- 81: Other Services (expcept Public Administration)

While the rank of these industries is somewhat suspect when considering which industries are most likely to be [impacted by the crisis](https://www.brookings.edu/research/how-local-leaders-can-stave-off-a-small-business-collapse-from-covid-19/), they cover the majority of what would be likely to be impacted by the crisis. 

```{r cleaning SBA census data and averaging predictor variables}
sba_census_merged <- sba_census_merged %>%
  mutate(EMP = as.double(EMP),
         ESTAB = as.double(EMP),
         PAYANN = as.double(PAYANN),
         RCPTOT = as.double(RCPTOT),
         PAYQTR1 = as.double(PAYQTR1)) %>%
  group_by(state, county, GEO_ID, NAICS2012) %>%
  summarize(
    emp = mean(EMP),
    estab = mean(ESTAB),
    payann = mean(PAYANN),
    rcptot = mean(RCPTOT),
    payqrt1 = mean(PAYQTR1))
 
sba_census_merged <- sba_census_merged%>%
  rename(countyFIPS = county,
         stateFIPS = state)
```

We include our targed predictor variables from the census surveys taken in 2007, 2012, and 2017 and average them such that we recieve a 10-year average for each variable. We should note that there is no first quarter payroll included in 2007 data.

```{r reading in COVID cases/deaths}
#Adding in COVID cases/deaths 5.3.20:
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")

covid <- covid %>%
  mutate(stateFIPS = substr(fips, 1, 2), 
         countyFIPS = substr(fips, 3, 5))

covid <- covid %>%
  group_by(stateFIPS, countyFIPS) %>%
  summarise(covid_deaths = sum(deaths),
            covid_cases = sum(cases))
```

To further tie the prediction to the COVID crisis we also include data provided by the New York Times on their public [GitHub page](https://github.com/nytimes/covid-19-data), which includes the count of COVID cases and deaths by county. We have to manipulate the data somewhat to total the data over time to get an accurate total per county, which we then merge with our census data.

```{r merging COVID and Census Data into single dataframe}
#merging onto sba_census data:
predictors <- sba_census_merged %>%
  left_join(covid, by = c("stateFIPS", "countyFIPS"))
```

With all that done we are then able to merge our SBA disaster loan data with our combined COVID/Census data to create a final datafram for machine learning analysis. 

```{r merging census and SBA disaster datasets into single dataframe}
#merging county and disaster numbers:
sba_merged_final <- sba_merged_all %>% 
  left_join(predictors, by = c("countyFIPS" = "countyFIPS",
                               "stateFIPS" = "stateFIPS"))

#rename some target variables:
names(sba_merged_final)[1] <- "SBA_Disaster_Number" 
names(sba_merged_final)[6] <- "Total_Verified_Loss"
names(sba_merged_final)[7] <- "Verified_Loss_Content"
names(sba_merged_final)[8] <- "Total_Approved_Loan_Amount"
names(sba_merged_final)[9] <- "Approved_Amount_Content"
names(sba_merged_final)[10] <- "Average_Amount_EIDL"
```

```{Mbay extra chunk}
#adding column for major disaster variable:
agg_disasters <- sba_merged_final %>%
  group_by(`SBA Disaster Number`) %>%
  summarise(sum(`Total Approved Loan Amount`)) %>%
  drop_na()

names(agg_disasters)[2] <- "Total_Loans"

Baysian_Disasters <- agg_disasters %>%
  filter(Total_Loans > quantile(Total_Loans, 0.99))

#Adding variable for top 1% of disasters
x = c("TX-00487", "FL-00130", "NY-00130", "PR-00031", "PR-00031", "	NJ-00033")
sba_2017 <- sba_2017 %>%
  mutate(
MBay_effect = ifelse(sba_2017$`SBA Disaster Number`%in% x, 1 ,0))
```

```{r checking out correlation}
#Finding correlation
sba_corr <- sba_merged_final %>%
  select(total_county_eidl_loan, avg_county_eidl_loan, total_county_approved_content_loan, total_county_verified_content_loan, avg_county_approved_content_loan, avg_county_verified_content_loan, MBay_effect, emp, payann, rcptot, payqrt1, estab, covid_cases, covid_deaths)

cor(na.omit(sba_corr))
```

```{r checking out dependent variable, echo = F}
#total_county_verified_content_loan
p1 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_verified_content_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "total verified loss")

#avg_county_verified_content_loan
p2 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_verified_content_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. content loan")

#total_county_approved_content_loan
p3 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_approved_content_loan))) +
  geom_histogram() +
  my_theme +
  labs(x = "total content loan")

#avg_county_approved_content_loan
p4 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_approved_content_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. content loan")


#total_county_eidl_loan
p5 <- ggplot(sba_merged_final, mapping = aes(x = log(total_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "total EIDL loan")


#avg_county_eidl_loan
p6 <- ggplot(sba_merged_final, mapping = aes(x = log(avg_county_eidl_loan))) +
  geom_histogram(binwidth = 0.35) +
  my_theme +
  labs(x = "avg. EIDL loan")

```

```{r dependent grid, warning = F}
grid.arrange(p1, p2, p3, p4, p5, p6, nrow = 2)
```

```{r checking out independent variables, echo = F}
#number of employees:
p7 <- ggplot(sba_merged_final, mapping = aes(x = log(emp))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#number of establisments:
p8 <- ggplot(sba_merged_final, mapping = aes(x = log(estab))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#annual payroll:
p9 <- ggplot(sba_merged_final, mapping = aes(x = log(payann))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#payroll in the first quarter:
p10 <- ggplot(sba_merged_final, mapping = aes(x = log(payqrt1))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#sales or revenue:
p11 <- ggplot(sba_merged_final, mapping = aes(x = log(rcptot))) +
  geom_histogram(binwidth = 0.35) +
  my_theme

#Micheal Bay effect:
p12 <- ggplot(sba_merged_final, mapping = aes(x = MBay_effect)) +
  geom_histogram(binwidth = 0.35) +
  my_theme
```

```{r independent grid, warning = F}
grid.arrange(p7, p8, p9, p10, p11, p12, nrow = 2)
```

## Model specifications

**Algorithm Considerations: **

We think it would be good to have a couple options to help see which might help the model fit better. Specifically:

- Random Forests
- Classic CART option
- Linear Regression 

We have some missing data and need to fill them in with machine learning model: 

```{checking on missing values}
table(is.na(sba_merged_final$total_county_eidl_loan))
table(is.na(sba_merged_final$avg_county_eidl_loan))
```

Model formulas:

 - avg_county_eidl_loan ~ avg_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect

- total_county_eidl_loan ~ total_county_verified_content_loan + emp + payann + rcptot + payqrt1 + estab + covid_cases + covid_deaths + MBay_effect

```{r pre-process for CART and linear}
#pre-process for CART and linear:
sba_merged_model <- sba_merged_final %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  sample_n(50000) 

set.seed(seed = 20200428)

split <- initial_split(sba_merged_model, prop = 0.80)
SBA_train <- training(split)
SBA_test <- testing(split)
SBA_resamples <- vfold_cv(data = SBA_train,
                          v = 10)
```

```{r linear and CART regression algorithm, warning = F}
#Compare RMSE:

SBA_resamples <- SBA_resamples %>%
  mutate(
    rmse_CARTavg = map_dbl(.x = splits,
                         .f = ~model_CART_avg(split = .x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_linearavg = map_dbl(.x = splits,
                           .f = ~model_linear_avg(.x,
                                        formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect)),
    rmse_CARTtotal = map_dbl(.x = splits,
                         .f = ~model_CART_total(split = .x,
                                          formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                            emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                            covid_deaths + MBay_effect)),
    rmse_lineartotal = map_dbl(.x = splits,
                           .f = ~model_linear_total(.x,
                                              formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                                emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                                covid_deaths + MBay_effect)))
    
  
SBA_resamples %>%
  select(id, rmse_CARTavg, rmse_linearavg, rmse_CARTtotal, rmse_lineartotal)
                        
#SBA_resamples %>%
#  pivot_longer(-id) %>%
#  ggplot(aes(x = id, 
#             y = value,
#             color = name,
#             group = name)) +
#    geom_point() +
#  geom_line() +
#  scale_y_continuous(limits = c(0, NA))

SBA_resamples %>%
  summarize(mean(rmse_linearavg),
            mean(rmse_CARTavg),
            mean(rmse_CARTtotal),
            mean(rmse_lineartotal))

```
clear that the averages with CART do better. I'd bet that random forests would do even better. 

``` {r setting seed and splitting into training and testing}
#pre-process:
sba_merged_model_rf <- sba_merged_final %>%
 select(avg_county_eidl_loan, total_county_eidl_loan, total_county_verified_content_loan, payann, payqrt1, emp, estab, rcptot, MBay_effect, covid_cases, covid_deaths) %>%
  na.omit(sba_merged_model_rf) %>%
  sample_n(50000)

set.seed(seed = 20200428)

split_rf <- initial_split(sba_merged_model_rf, prop = 0.80)
SBA_train_rf <- training(split_rf)
SBA_test_rf <- testing(split_rf)
```

```{r RandomForest algorithm for avg_county_eidl_loan}
n_features <- length(setdiff(names(SBA_train_rf), "avg_county_eidl_loan"))

sba_rf_avg <- ranger(
  formula = avg_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train_rf,
  num.tree = n_features*10,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(avg_rmse <- sqrt(sba_rf_avg$prediction.error))
```

```{r RandomForest algorithm for total_county_eidl_loan}
n_features1 <- length(setdiff(names(SBA_train_rf), "total_county_eidl_loan"))

sba_rf_total <- ranger(
  formula = total_county_eidl_loan ~ total_county_verified_content_loan + 
                                          emp + payann + rcptot + payqrt1 + estab + covid_cases + 
                                          covid_deaths + MBay_effect,
  data = SBA_train_rf,
  num.tree = n_features1*10,
  mtry = floor(n_features1 / 3),
  respect.unordered.factors = "order",
  seed = 20200428
)

(total_rmse <- sqrt(sba_rf_total$prediction.error))
```

How to calculate PPP loans: 
https://www.sba.gov/sites/default/files/2020-04/How-to-Calculate-Loan-Amounts.pdf

